# ğŸ“Œ Retrieval-Augmented Generation (RAG)

<p align="center">
  <img src="image.png" width="100%">
</p>

Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant external knowledge and injecting it into the prompt before generation. 

This approach improves factual accuracy, grounding, and significantly reduces hallucinations.

---

## ğŸ”¹ Architecture

This implementation currently covers data ingestion and vector indexing only.
The pipeline prepares documents for semantic search by converting them into dense vector embeddings stored in FAISS.

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Raw Documents (data/)  â”‚
            â”‚  PDF / TXT / CSV inputs  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚      data_ingestion.py   â”‚
            â”‚  load â†’ clean â†’ chunk    â”‚
            â”‚  embed documents         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ 
                          â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   FAISS Vector Store     â”‚
            â”‚  index dense embeddings  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            
Components
	â€¢	Raw Documents (data/)
	â€¢	Source files used for knowledge ingestion (PDF, text, CSV).
	â€¢	Data Ingestion (data_ingestion.py)
	â€¢	Loads documents
	â€¢	Cleans and preprocesses text
	â€¢	Splits text into chunks
	â€¢	Generates dense embeddings using Sentence-Transformers
	â€¢	Vector Store (vector_store_faiss.py)
	â€¢	Creates and manages FAISS index
	â€¢	Stores dense vector representations
	â€¢	Enables efficient similarity search (retrieval layer to be added later)
---


## ğŸ”¹ Ingestion

Raw data is processed, transformed, and stored to enable fast and accurate semantic search.

### Ingestion Pipeline
* Load documents (PDF / Text / CSV)
* Clean and preprocess text
* Chunk documents into semantically meaningful segments
* Generate dense vector embeddings
* Store embeddings in FAISS for efficient similarity search

---

## ğŸ”¹ Retrieval (Planned)

The retrieval stage will be added in the next phase of the project.

### Planned Retrieval Flow
* Convert user query into an embedding
* Perform similarity search using FAISS
* Retrieve Top-K most relevant document chunks
* Pass retrieved context to an LLM for grounded response generation

---

## ğŸ”¹ Tech Stack

### Embeddings
- Sentence-Transformers
  * BGE
  * E5
  * SBERT

### Vector Store
* FAISS (Facebook AI Similarity Search)

---

## ğŸ”¹ Core Files

- `data_ingestion.py`  
  Handles document loading, cleaning, chunking, and embedding generation

- `vector_store_faiss.py`  
  Manages FAISS indexing and similarity search

- `data/`  
  Contains raw input documents

- `requirements.txt`  
  Project dependencies

---
